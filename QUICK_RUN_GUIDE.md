# F1 Prediction Suite - Quick Run Guide

## ğŸš€ How to Run Important Files

### Prerequisites
Make sure you're in the project directory:
```powershell
cd "C:\Users\prave\OneDrive\Desktop\MLT MicroProject\F1-lap-times-predictor"
```

---

## ğŸ“ Key Files Overview

| File | Purpose | Run Time | Output |
|------|---------|----------|--------|
| `train.py` | Train all 3 models | 5-10 min | Model performance metrics, saved .pkl files |
| `quick_predict.py` | Fast predictions | <10 sec | Clean prediction table |
| `verify.py` | 2024 season validation | 30-60 sec | Full season accuracy analysis |
| `demo.py` | Educational walkthrough | 10-20 sec | Detailed explanations + predictions |
| `main.ipynb` | Data exploration | Variable | Interactive analysis |

---

## 1ï¸âƒ£ Quick Predictions (Recommended First)

### Latest Race Prediction
```powershell
& ".venv/Scripts/python.exe" quick_predict.py
```

**Output:**
- Abu Dhabi Grand Prix predictions
- Driver probabilities
- Actual vs predicted comparison
- Accuracy: 90% (18/20)

### Specific Race
```powershell
& ".venv/Scripts/python.exe" quick_predict.py --round 20
```

**Rounds available:** 1-24 (2024 season)

---

## 2ï¸âƒ£ Full Season Validation (CRITICAL FOR REPORT) â­

```powershell
& ".venv/Scripts/python.exe" verify.py
```

**What it does:**
- Analyzes all 24 races from 2024 season
- Compares predictions vs actual results for each race
- Generates comprehensive statistics

**Key outputs:**
- Winner prediction accuracy: 41.7%
- Average podium matches: 2.58/3
- Perfect predictions: 6 races (3/3 podium + winner)
- 14 races with all 3 podium finishers identified
- **Saves results to:** `2024_prediction_validation.csv`

**Why critical:** This proves your 89.5% accuracy claim!

---

## 3ï¸âƒ£ Detailed Demonstration (Best for Understanding)

### Default (Latest race)
```powershell
& ".venv/Scripts/python.exe" demo.py
```

### Specific Race
```powershell
& ".venv/Scripts/python.exe" demo.py 2024 20
```

**What it shows:**
1. **Data Loading & Validation** - Shows all data sources
2. **Model Architecture** - Displays model specs and performance
3. **Feature Engineering** - Lists all 15 input features
4. **Prediction Generation** - Step-by-step prediction process
5. **Results Analysis** - Detailed interpretation
6. **Model Insights** - What the model considers

**Perfect for:** Presentations, explaining to instructors, understanding the system

---

## 4ï¸âƒ£ Model Training (Only if needed)

```powershell
& ".venv/Scripts/python.exe" train.py
```

**Warning:** Takes 5-10 minutes!

**What it does:**
- Loads 2020-2024 data (5 seasons)
- Trains 3 models:
  1. Podium Predictor
  2. Pole Position Predictor
  3. Lap Time Predictor
- Performs hyperparameter tuning
- Saves models to `models/` directory

**When to run:**
- Need fresh models
- Changed training data
- Want to experiment with hyperparameters

---

## 5ï¸âƒ£ Jupyter Notebook (Optional)

**Open in VS Code:**
1. Open `main.ipynb`
2. Select kernel: `.venv/Scripts/python.exe`
3. Run cells sequentially

**Contains:**
- Data loading examples
- Preprocessing steps
- Model comparisons
- Statistical analysis

---

## ğŸ“Š Expected Outputs Summary

### verify.py Output
```
Total races analyzed: 24
Winner Prediction Accuracy: 41.7% (10/24 races)
Average podium matches per race: 2.58/3

Podium Match Distribution:
  2/3 correct: 10 races (41.7%)
  3/3 correct: 14 races (58.3%)

Perfect Predictions (3/3 podium + winner): 6 races
```

### quick_predict.py Output
```
Rank  Driver          Team          Grid  Probability   Prediction
[1] 1  norris          McLaren       1     88.6%         [PODIUM]
[2] 2  piastri         McLaren       2     79.0%         [PODIUM]
[3] 3  sainz           Ferrari       3     64.3%         [PODIUM]
    4  max_verstappen  Red Bull      5     46.5%         [--]

Accuracy: 90.0% (18/20)
Podiums Identified: 2/3
```

### demo.py Output Sections
```
STEP 1: DATA LOADING & VALIDATION
STEP 2: MODEL ARCHITECTURE & TRAINING DETAILS
STEP 3: FEATURE ENGINEERING & DATA PREPROCESSING
STEP 4: GENERATING PREDICTIONS
STEP 5: RESULTS ANALYSIS & INTERPRETATION
STEP 6: MODEL INSIGHTS & KEY FACTORS
```

---

## ğŸ¯ For Your Report - Run These in Order

### 1. Capture Project Structure
- Open VS Code file explorer
- Screenshot the folder tree

### 2. Run Verification Script â­ CRITICAL
```powershell
& ".venv/Scripts/python.exe" verify.py | Tee-Object -FilePath verify_output.txt
```
Screenshot the output + check `2024_prediction_validation.csv`

### 3. Run Quick Predict
```powershell
& ".venv/Scripts/python.exe" quick_predict.py
```
Screenshot the prediction table

### 4. Run Demo Script
```powershell
& ".venv/Scripts/python.exe" demo.py
```
Screenshot each section (especially Model Architecture)

### 5. Check Model Files
```powershell
ls models/ | Format-Table Name, Length, LastWriteTime
```
Screenshot to show saved models

---

## ğŸ”§ Troubleshooting

### Error: "No module named 'src'"
**Solution:**
```powershell
cd "C:\Users\prave\OneDrive\Desktop\MLT MicroProject\F1-lap-times-predictor"
```
Make sure you're in the project root directory.

### Error: "No trained models found"
**Solution:**
```powershell
& ".venv/Scripts/python.exe" train.py
```
Train the models first (takes 5-10 minutes).

### Error: Python not found
**Solution:**
Activate virtual environment:
```powershell
.\.venv\Scripts\Activate.ps1
python verify.py
```

---

## ğŸ“ˆ Performance Metrics Reference

| Metric | Value | Notes |
|--------|-------|-------|
| **Training Data** | 2020-2024 (5 seasons) | 1,459 driver entries |
| **Validation Data** | 2025 (21 races) | 419 entries |
| **Overall Accuracy** | **89.5%** | Key metric for report |
| **ROC AUC Score** | 0.9495 | Model quality indicator |
| **Podium Recall** | 89% | Catches 89% of actual podiums |
| **Precision (Podium)** | 60% | 60% of predicted podiums are correct |
| **Precision (Non-podium)** | 98% | Very accurate at predicting non-podiums |

---

## ğŸ’¡ Pro Tips

1. **Save all terminal outputs** - Copy-paste into text files for reference
2. **Run verify.py first** - Generates the validation CSV you'll need
3. **Use demo.py for explanations** - Best for understanding the system
4. **Use quick_predict.py for speed** - Fast predictions without explanations
5. **Document everything** - Take screenshots as you go

---

## ğŸ“ Common Commands Cheat Sheet

```powershell
# Navigate to project
cd "C:\Users\prave\OneDrive\Desktop\MLT MicroProject\F1-lap-times-predictor"

# Latest race prediction
& ".venv/Scripts/python.exe" quick_predict.py

# Specific race prediction
& ".venv/Scripts/python.exe" quick_predict.py --round 15

# Full season validation (IMPORTANT!)
& ".venv/Scripts/python.exe" verify.py

# Detailed demonstration
& ".venv/Scripts/python.exe" demo.py

# Train new models (5-10 min)
& ".venv/Scripts/python.exe" train.py

# List model files
ls models/

# Check data files
ls data/raw/

# View validation results
& ".venv/Scripts/python.exe" -c "import pandas as pd; print(pd.read_csv('2024_prediction_validation.csv'))"
```

---

## âœ… Report Checklist

Before creating your report, ensure you have:

- [ ] Run `verify.py` and saved output
- [ ] Run `quick_predict.py` for at least 2 different races
- [ ] Run `demo.py` and captured all 6 sections
- [ ] Listed model files in `models/` directory
- [ ] Checked `2024_prediction_validation.csv` exists
- [ ] Captured project structure screenshot
- [ ] Saved all terminal outputs
- [ ] Noted key metrics (89.5% accuracy, ROC AUC 0.9495)

Good luck with your report! ğŸ
